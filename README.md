ğŸ§  LLM Evaluation & Benchmarking Framework

A lightweight and extensible framework for evaluating and benchmarking Large Language Models (LLMs) using LLM-as-a-Judge, structured scoring, and multi-model comparison.

This project simulates real-world LLM evaluation workflows used in AI product development, model selection, and prompt optimization.

ğŸš€ Features

âœ… Multi-model evaluation (GPT-4o, GPT-3.5, etc.)

âœ… LLM-as-a-judge scoring

âœ… Evaluation metrics:

Relevance

Hallucination

Instruction adherence

Response latency

âœ… Weighted score aggregation

âœ… Automatic best-model selection

âœ… CSV-based result export

âœ… Clean, modular Python architecture

ğŸ“ Project Structure

llm-eval-benchmark/
â”‚
â”œâ”€â”€ evaluator/
â”‚   â”œâ”€â”€ llm_client.py        # LLM API calls
â”‚   â”œâ”€â”€ evaluator.py         # Core evaluation logic
â”‚   â”œâ”€â”€ judge.py             # LLM-as-a-judge scoring
â”‚   â”œâ”€â”€ scorer.py            # Metric helpers
â”‚   â””â”€â”€ aggregator.py        # Score aggregation & ranking
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ factual.json
â”‚   â””â”€â”€ instructions.json
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ results.csv          # Raw evaluation output
â”‚   â””â”€â”€ summary.csv          # Aggregated scores
â”‚
â”œâ”€â”€ runner.py                # Main execution script
â”œâ”€â”€ config.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


ğŸ§  How It Works

Prompts are loaded from JSON files

Each prompt is evaluated across multiple LLMs

Responses are scored using:

LLM-as-a-judge

Structured evaluation logic

Scores are aggregated using weighted metrics

The best-performing model is automatically selected

ğŸ“Š Evaluation Metrics
Metric	Description
Relevance	How accurately the response answers the prompt
Hallucination	Whether the response contains incorrect facts
Instruction	Adherence to prompt constraints
Latency	Response time of the model

Each metric contributes to a final weighted score.

â–¶ï¸ How to Run
1ï¸âƒ£ Install dependencies
pip install -r requirements.txt

2ï¸âƒ£ Set environment variable
Create a .env file:
OPENROUTER_API_KEY=your_api_key_here

3ï¸âƒ£ Run evaluation
python runner.py

ğŸ“ˆ Output
results/results.csv

Contains raw model responses and individual scores.

results/summary.csv

Contains aggregated results and final model rankings.

Example:

Model	Final Score
openai/gpt-4o-mini	4.54
openai/gpt-3.5-turbo	3.78
ğŸ† Best Model Selection

The framework automatically selects the best-performing model based on:

Accuracy

Hallucination risk

Instruction adherence

Latency

This mirrors how LLMs are evaluated in production systems.

ğŸ§­ Roadmap
ğŸ”¹ Upcoming Enhancements

ğŸ“Š Streamlit dashboard for visual analysis

ğŸ” Prompt variation testing

ğŸ§  RAG-based evaluation support

âš™ï¸ Config-driven scoring (YAML)

ğŸ”„ CI-based evaluation pipeline

ğŸ’¡ Why This Project Matters

This project demonstrates:

Practical LLM evaluation techniques

Model benchmarking and comparison

Prompt engineering analysis

Real-world LLMOps thinking

Production-style project structuring

ğŸ‘¨â€ğŸ’» Author

Akshay Gwasikoti
AI Automation | LLM Evaluation | Applied AI Engineering
